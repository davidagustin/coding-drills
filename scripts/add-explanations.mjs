import fs from 'fs';

const filePath = '/Users/davidagustin/Desktop/codingfolder/coding-drills/lib/exercises/javascript.ts';
const content = fs.readFileSync(filePath, 'utf8');
const lines = content.split('\n');

// Map of exercise ID -> explanation text (all 163 exercises)
const explanations = {
  'js-skip-every-other': `Stepping through an array at a fixed stride is one of the most fundamental iteration patterns in programming. Instead of visiting every element, you increment the loop counter by 2 (or any step size), effectively sampling every other element.\\n\\nThe key insight is that index manipulation in the loop header (i += 2 instead of i++) controls which elements you visit. This gives O(n/2) time complexity, which simplifies to O(n), and O(n/2) space for the output array.\\n\\nThis pattern appears everywhere: processing alternating rows/columns in a matrix, downsampling signals in audio processing, handling even/odd indexed items differently, and interleaving operations.`,

  'js-reverse-iteration': `Reverse iteration teaches you to think about arrays bidirectionally. By starting at the last index (arr.length - 1) and decrementing, you naturally produce a reversed copy without needing a built-in method.\\n\\nThe boundary condition is the most important detail: initialize i to arr.length - 1, and loop while i >= 0. Off-by-one errors here are the most common source of bugs in index-based loops. The time complexity is O(n) and space is O(n) for the output.\\n\\nReverse traversal is essential when you need to process elements from the end (like evaluating postfix expressions), when removing elements during iteration (to avoid index shifting), or when implementing in-place reversal algorithms with two pointers.`,

  'js-step-iteration': `Variable-step iteration generalizes the fixed-increment pattern, letting you select elements at any regular interval. By parameterizing the step size, you create a reusable pattern that works for downsampling, batch selection, and strided access.\\n\\nThe loop structure is for(let i = 0; i < arr.length; i += step), which naturally handles edge cases: if step exceeds the array length, only the first element is returned. Time complexity is O(n/step) and space is O(n/step) for the output.\\n\\nThis concept extends directly to NumPy-style array slicing (arr[::step]), database OFFSET/LIMIT pagination, and processing every kth frame in video analysis.`,

  'js-nested-loop-matrix': `Nested loops for matrix traversal establish the row-major access pattern used throughout computing. The outer loop selects each row, and the inner loop visits each column within that row, producing elements in reading order.\\n\\nThe time complexity is O(m * n) where m is the number of rows and n is the number of columns. This is optimal since you must visit every element. Space is O(m * n) for the flattened output.\\n\\nRow-major traversal is cache-friendly in most languages because arrays are stored in row-major order in memory. This pattern forms the basis for image pixel processing, spreadsheet cell iteration, game board updates, and the inner kernel of matrix multiplication algorithms.`,

  'js-prime-generation': `Prime generation via trial division is one of the oldest algorithms in mathematics, and implementing it teaches nested-loop control flow with early termination. For each candidate number, you test divisibility by all integers from 2 up to its square root.\\n\\nThe square-root optimization is the key insight: if n has a factor larger than sqrt(n), it must also have a corresponding factor smaller than sqrt(n). This reduces the inner loop from O(n) to O(sqrt(n)) checks per candidate, making the overall algorithm O(n * sqrt(n)).\\n\\nFor generating primes up to large N, the Sieve of Eratosthenes is preferred at O(n log log n). But trial division is valuable for understanding prime testing, and primes themselves are central to RSA encryption, hash table sizing, and number theory interview problems.`,

  'js-fibonacci-iterative': `The Fibonacci sequence is the canonical example of a recurrence relation that can be computed iteratively. Each number is the sum of the two before it, building up from known base cases fib(0)=0 and fib(1)=1.\\n\\nThe iterative approach runs in O(n) time and O(n) space (for storing the full sequence). This is dramatically better than naive recursion which would be O(2^n) due to recomputing the same subproblems exponentially many times.\\n\\nThis bottom-up iteration pattern is the foundation of dynamic programming. Once you recognize that a problem has overlapping subproblems and can be built from smaller solutions, you can apply the same iterative table-filling strategy to problems like climbing stairs, coin change, and longest common subsequence.`,

  'js-fibonacci-recursive': `Naive recursive Fibonacci is intentionally inefficient, and understanding why is the gateway to dynamic programming. Each call branches into two subcalls, creating an exponential call tree of O(2^n) nodes because fib(k) gets recomputed at every level.\\n\\nThe recursive structure directly mirrors the mathematical definition: fib(n) = fib(n-1) + fib(n-2), with base cases fib(0) = 0 and fib(1) = 1. This makes the code elegant but impractical for n > 40 or so.\\n\\nThe real value of this exercise is motivational: once you see the exponential blowup, memoization becomes obvious. Caching results of fib(k) in a hash map collapses the tree to O(n) calls. This insight drives all of dynamic programming and appears in countless interview problems.`,

  'js-factorial-recursive': `Factorial is the simplest non-trivial recursive function. It has exactly one base case (n <= 1 returns 1) and one recursive step (n * factorial(n-1)), making it the ideal introduction to recursive thinking.\\n\\nThe call stack grows to depth n, giving O(n) time and O(n) space. Each frame multiplies the returned value by the current n as it unwinds. This linear recursion pattern (single recursive call per frame) is the simplest form, contrasted with tree recursion (like Fibonacci) which branches.\\n\\nFactorial itself computes n! = n * (n-1) * ... * 1, the number of ways to arrange n distinct items. It appears in permutation counting (n!), combination formulas (n! / (k!(n-k)!)), probability calculations, and Taylor series expansions.`,

  'js-sum-recursive': `Recursive array summation teaches the fundamental pattern of decomposing a collection into "first element" plus "rest of collection." The base case is an empty array (sum = 0), and the recursive step is arr[0] + sumArray(arr.slice(1)).\\n\\nWhile this runs in O(n) calls, each slice creates a new array, making the actual complexity O(n^2) in both time and space due to copying. In practice, you would pass an index parameter instead of slicing to achieve true O(n) performance.\\n\\nThis head-plus-tail decomposition is the foundational pattern of functional programming languages like Haskell and Lisp. It generalizes to recursive map, filter, reduce, and tree traversals where you process the current element and recurse on the remainder.`,

  'js-dfs-tree': `Pre-order DFS visits the current node first, then recursively processes the left and right subtrees. This "visit before descending" order means the root always appears first in the output.\\n\\nThe recursion naturally uses the call stack, giving O(h) space complexity where h is the tree height (O(log n) for balanced, O(n) for skewed). Time complexity is O(n) since every node is visited exactly once.\\n\\nPre-order traversal is used for serializing trees (the root-first order allows easy reconstruction), copying tree structures, generating prefix expressions from expression trees, and creating a printable representation of directory structures. It is the most intuitive DFS ordering and typically the first taught.`,

  'js-dfs-inorder': `In-order traversal visits the left subtree, then the current node, then the right subtree. For a Binary Search Tree, this produces elements in sorted ascending order, which is the key insight that makes in-order traversal so important.\\n\\nLike all DFS traversals, it runs in O(n) time and O(h) space on the call stack. The sorted output property makes in-order the natural choice for BST validation: if the output is not sorted, the tree violates BST invariants.\\n\\nBeyond validation, in-order traversal is used for range queries on BSTs (print all elements between a and b), computing the kth smallest element, converting a BST to a sorted doubly-linked list, and building balanced BSTs from sorted data.`,

  'js-bfs-tree': `BFS explores a tree level by level using a queue instead of recursion. You dequeue a node, process it, then enqueue its children. This guarantees that all nodes at depth d are visited before any node at depth d+1.\\n\\nTime complexity is O(n) and space complexity is O(w) where w is the maximum width of the tree (the largest number of nodes at any single level). For a complete binary tree, this is O(n/2) = O(n) at the last level.\\n\\nBFS is essential when the solution depends on distance from the root: finding the minimum depth, level-order grouping, connecting nodes at the same level, and shortest path in unweighted graphs. The queue-based iterative pattern avoids stack overflow on deep trees.`,

  'js-binary-search': `Binary search is the most important search algorithm, reducing a sorted array lookup from O(n) to O(log n) by eliminating half the search space each step. Compare the target to the middle element, then search the left or right half.\\n\\nThe critical details are correct boundary management (lo = 0, hi = arr.length - 1, loop while lo <= hi) and midpoint calculation (using (lo + hi) >>> 1 to avoid integer overflow). Getting these wrong is the source of most binary search bugs.\\n\\nBinary search extends far beyond simple array lookup. It applies to any monotonic function: searching on the answer space, finding boundaries, minimizing/maximizing with a feasibility check, and is the backbone of database index lookups and git bisect.`,

  'js-linked-list-traverse': `Linked list traversal teaches pointer-following iteration, a fundamentally different pattern from array indexing. You start at the head node and follow .next pointers until reaching null, processing each node along the way.\\n\\nThe traversal runs in O(n) time and O(1) extra space (or O(n) if collecting values into an array). Unlike arrays, you cannot jump to the ith element; you must walk from the head, making random access O(n).\\n\\nLinked list traversal is the foundation for all linked list operations: searching, insertion, deletion, reversal, and cycle detection. Understanding pointer manipulation here prepares you for more complex structures like doubly-linked lists, skip lists, and graph adjacency lists.`,

  'js-stack-operations': `Stack operations teach LIFO (Last-In-First-Out) ordering, where push adds to the top and pop removes from the top. This simple interface enables powerful algorithmic patterns that appear throughout computer science.\\n\\nAll stack operations run in O(1) amortized time using an array (push/pop at the end). The key invariant is that only the most recently added element is accessible, which naturally models nested or reversible operations.\\n\\nStacks are everywhere: the JavaScript call stack, undo/redo systems, bracket matching, expression evaluation (postfix notation), DFS traversal (explicit stack replaces recursion), browser back-button history, and parsing nested structures like HTML or JSON.`,

  'js-generate-range': `Range generation creates a sequence of evenly-spaced numbers, similar to Python's range() or lodash's _.range(). The key is computing how many elements to generate: Math.ceil((end - start) / step).\\n\\nThis utility runs in O(n) time where n is the number of elements generated. It handles edge cases like negative steps for descending ranges and returns an empty array when the step direction contradicts the start-to-end direction.\\n\\nRange is a foundational utility for functional programming, replacing C-style for loops with declarative iteration. It enables patterns like range(0, n).map(fn), powers test case generation, and is the basis for more complex sequence generators like linspace in numerical computing.`,

  'js-generate-subsets': `Subset generation (power set) is the foundation of combinatorial enumeration. For each element, you have a binary choice: include it or exclude it. This creates 2^n total subsets, which you can generate recursively by branching at each element.\\n\\nThe recursive approach builds subsets depth-first, adding one element at each level. Time complexity is O(2^n) since that is the number of subsets, and each is copied into the result. Space complexity is O(n) for the recursion stack depth.\\n\\nSubset generation is used in brute-force optimization (try all subsets to find the best), feature selection in machine learning, test case enumeration, and as a subroutine in algorithms that need to examine all possible combinations of a set.`,

  'js-generate-combinations': `Combination generation (n choose k) selects k items from n without regard to order, using backtracking with a forward-only start index to avoid duplicates. You build combinations incrementally, pruning when the current combination reaches size k.\\n\\nThe number of results is C(n,k) = n! / (k!(n-k)!). The backtracking approach explores each possibility by adding element i, recursing with start = i+1, then removing element i. Time complexity is O(C(n,k) * k) accounting for copying each result.\\n\\nCombinations appear in lottery probability calculations, selecting committee members, choosing features for a model, generating test input pairs, and as the core primitive for many constraint satisfaction problems in interviews.`,

  'js-generate-permutations': `Permutation generation produces all possible orderings of a collection, where the arrangement matters. For n elements, there are n! permutations. The recursive approach picks each element as the "first" and permutes the remaining elements.\\n\\nThe algorithm branches n ways at the first level, n-1 at the second, and so on, naturally producing n! results. Each permutation is built by concatenating the chosen element with the permutation of the rest. Time complexity is O(n! * n) accounting for array operations.\\n\\nPermutations are central to brute-force search over arrangements: scheduling jobs, finding optimal routes (TSP), generating anagrams, testing all possible configurations, and cryptographic key enumeration. They are also the basis for understanding group theory in mathematics.`,

  'js-cartesian-product': `The Cartesian product of two sets pairs every element from the first with every element from the second. Using flatMap for the outer iteration and map for the inner produces all pairs in a concise functional style.\\n\\nThe result size is |A| * |B|, making both time and space complexity O(|A| * |B|). flatMap is crucial here because map alone would produce nested arrays, while flatMap flattens one level to give a clean array of pairs.\\n\\nCartesian products generate grid coordinates from row and column ranges, enumerate all test case combinations from parameter lists, create join results in databases (cross join), and form the basis for multi-dimensional combinatorial problems.`,

  'js-cartesian-n-arrays': `N-ary Cartesian product generalizes pair generation to any number of input arrays using reduce with flatMap. Starting from [[]], each new array extends every existing tuple with every value from the new array.\\n\\nThe result size is the product of all array lengths. The reduce approach is elegant: the accumulator holds all partial tuples, and each iteration extends them by one dimension. Time and space are O(product of all lengths * number of arrays).\\n\\nThis pattern is used in configuration enumeration (all combinations of settings), constraint satisfaction solvers, exhaustive test generation from parameter spaces, and SQL-style multi-table cross joins. It demonstrates how reduce can build complex structures incrementally.`,

  'js-binomial-coefficient': `The binomial coefficient C(n,k) counts the number of ways to choose k items from n, computed as n! / (k! * (n-k)!). The iterative multiplication approach avoids computing large factorials by canceling terms incrementally.\\n\\nThe key optimization is computing C(n,k) = (n * (n-1) * ... * (n-k+1)) / (k * (k-1) * ... * 1), which uses only k multiplications and divisions. This avoids overflow for moderate values and runs in O(k) time with O(1) space.\\n\\nBinomial coefficients appear as entries in Pascal's triangle, coefficients in polynomial expansion, probability calculations in statistics, and counting paths in grids. They are fundamental to combinatorics and arise in algorithm analysis whenever you need to count selections.`,

  'js-basic-memoize': `Memoization wraps a function with a cache so that repeated calls with the same argument return instantly from the cache instead of recomputing. This closure-based pattern is the runtime equivalent of dynamic programming.\\n\\nThe wrapper function checks a Map or object before calling the original function. If the argument has been seen before, it returns the cached result in O(1). Otherwise, it computes, stores, and returns the value. Space grows with the number of unique inputs cached.\\n\\nMemoization is used to optimize recursive algorithms (Fibonacci, tree computations), cache expensive API responses, avoid redundant DOM calculations in React (useMemo), and speed up any pure function that is called repeatedly with the same arguments.`,

  'js-memoize-multi-arg': `Multi-argument memoization extends basic memoization by serializing all arguments into a single cache key. The simplest approach uses JSON.stringify(args), though custom key functions offer better performance for specific use cases.\\n\\nThe key challenge is that JavaScript objects and Maps can only use primitive keys efficiently. JSON.stringify converts arguments to a string representation, but this has edge cases with undefined, functions, and circular references. A production implementation might use a nested Map structure or WeakMap for object arguments.\\n\\nMulti-argument memoization is essential for caching database queries with multiple parameters, memoizing selectors in Redux (reselect), and optimizing any pure function of multiple variables in scientific computing or data processing pipelines.`,

  'js-memoize-fibonacci': `Memoized Fibonacci demonstrates the dramatic impact of caching on recursive algorithms. By storing previously computed fib(k) values, each subproblem is solved only once, collapsing the O(2^n) naive recursion to O(n) time.\\n\\nThe memoization wrapper intercepts each call: if fib(k) is in the cache, return it immediately; otherwise, compute it recursively (which itself benefits from the cache) and store the result. The call tree degenerates from an exponential tree into a linear chain.\\n\\nThis is the canonical example motivating dynamic programming. The same pattern applies to any recursive function with overlapping subproblems: grid path counting, edit distance, coin change, and tree computations. Recognizing when memoization applies is one of the most valuable algorithmic skills.`,

  'js-debounce': `Debounce delays function execution until a specified quiet period elapses after the last call. Each new call resets the timer, so the function only fires when calls stop arriving for the delay duration.\\n\\nThe implementation uses a closure to hold a timer ID. On each call, clearTimeout cancels any pending execution, and setTimeout schedules a new one. This gives O(1) per call, with the actual function executing at most once per burst of calls.\\n\\nDebounce is essential for search-as-you-type (wait until the user stops typing), window resize handlers (recalculate layout once resizing stops), auto-save features, and any scenario where you want to respond to the final intent rather than every intermediate action.`,

  'js-throttle': `Throttle ensures a function executes at most once per interval, regardless of how often it is called. Unlike debounce which waits for silence, throttle guarantees regular execution during continuous activity.\\n\\nThe implementation tracks whether a timer is active. The first call within an interval executes immediately and starts a timer. Subsequent calls during the interval are ignored. After the interval, the next call fires immediately again. Time complexity is O(1) per call.\\n\\nThrottle is ideal for scroll handlers (update position at most every 100ms), mousemove tracking, rate-limiting API calls, game loop input processing, and any event that fires continuously where you want periodic responses rather than eventual response.`,

  'js-once-function': `The once wrapper ensures a function executes only on its first invocation, returning the cached result for all subsequent calls. This uses a closure with a boolean flag and a stored result value.\\n\\nThe pattern is simple but powerful: check the flag before calling, set it before executing (to handle recursive cases), and store the return value. All subsequent calls skip execution and return the cached result in O(1) time.\\n\\nOnce is used for lazy initialization (compute expensive setup on first use), singleton patterns, database connection establishment, one-time event handlers (like initialization), and ensuring idempotent operations in distributed systems where retries might re-trigger the same function.`,

  'js-chunk-array': `Chunking splits an array into groups of a specified size, with the last chunk potentially smaller. The Array.from approach computes the number of chunks as Math.ceil(length/size) and slices each one.\\n\\nTime complexity is O(n) since each element is copied into exactly one chunk. The Array.from with a mapping function is a clean declarative pattern: it creates the chunk count and maps each index to a slice.\\n\\nChunking is used in pagination (show page k of results), batch API calls (send 100 records at a time), parallel processing (distribute work across workers), rendering large lists in virtual scrolling, and splitting data for map-reduce processing.`,

  'js-partition': `Partition splits an array into exactly two groups based on a predicate: elements where the predicate returns true, and elements where it returns false. Using reduce with a [[], []] accumulator is the canonical implementation.\\n\\nThe reduce processes each element exactly once in O(n) time. The conditional index (predicate(val) ? 0 : 1) elegantly routes each element to the correct sub-array. Space is O(n) for the two output arrays.\\n\\nPartition is a more informative version of filter because it preserves both the matching and non-matching elements. It is used for separating valid/invalid form data, A/B test group assignment, splitting training/test datasets, and any binary classification of a collection.`,

  'js-zip-arrays': `Zip combines two arrays by pairing elements at the same index, like a physical zipper merging two sides tooth by tooth. The result length equals the shorter input array, naturally handling mismatched lengths.\\n\\nThe Array.from approach creates pairs in O(min(m,n)) time. Using Math.min for the length handles the edge case cleanly, and the index-based pairing is straightforward.\\n\\nZip is fundamental for correlating parallel data: pairing keys with values to create objects, combining x-coordinates with y-coordinates to create points, merging column data into rows, and iterating over multiple collections simultaneously as in Python's zip().`,

  'js-unzip-pairs': `Unzip is the inverse of zip: it takes an array of pairs and separates them into two arrays. Two map operations extract the first and second elements respectively.\\n\\nTime complexity is O(n) with two passes over the data (one for each position). This could be done in a single pass with reduce, but the dual-map approach is clearer and the performance difference is negligible.\\n\\nUnzip is used for separating coordinate pairs into x and y arrays for charting, extracting keys and values from entries (inverse of Object.entries), splitting CSV column pairs, and decomposing any parallel data structure into its components for independent processing.`,

  'js-group-by': `GroupBy collects elements into buckets based on a key function, creating an object where each key maps to an array of elements that produced that key. It is one of the most used data transformation utilities.\\n\\nThe reduce implementation processes each element once in O(n) time. For each element, the key function determines the bucket, and the element is pushed to the corresponding array. Space is O(n) for the grouped output.\\n\\nGroupBy is ubiquitous in data processing: grouping transactions by date, users by role, products by category, log entries by severity level, and is the conceptual equivalent of SQL GROUP BY. Libraries like Lodash feature it prominently, and Array.prototype.groupBy is being added to JavaScript natively.`,

  'js-frequency-counter': `Frequency counting tallies how many times each element appears, producing a histogram as an object mapping values to counts. This fundamental pattern uses reduce to build the count map in a single pass.\\n\\nThe algorithm runs in O(n) time with O(k) space where k is the number of unique elements. The pattern (counts[key] = (counts[key] || 0) + 1) is idiomatic JavaScript for incrementing a possibly-undefined counter.\\n\\nFrequency counting is foundational: it solves anagram detection, finds the mode of a dataset, identifies duplicate elements, enables bucket sort, validates character distributions, and is the first step in many string and array interview problems like "most common element" or "first unique character."`,

  'js-sliding-window': `The sliding window technique maintains a fixed-size view over a sequence, advancing one element at a time. Rather than recomputing the window contents from scratch each step, you efficiently update by removing the outgoing element and adding the incoming one.\\n\\nCollecting all windows of size k takes O(n) time with O(k) per window for the slice operation. The overall space is O(n * k) for storing all windows, or O(k) if processing each window immediately.\\n\\nSliding windows power moving averages in time series analysis, substring search algorithms (like Rabin-Karp), network packet inspection, real-time analytics over data streams, and are the conceptual basis for convolutional filters in signal processing and neural networks.`,

  'js-flatten-deep': `Deep flattening recursively unwraps nested arrays to a specified depth, collecting all leaf values into a single-level array. The depth parameter controls how many levels of nesting to remove.\\n\\nThe recursive approach processes each element: if it is an array and depth > 0, recurse with depth-1; otherwise, include it directly. Time complexity is O(n) where n is the total number of elements across all nesting levels. Stack depth equals the maximum nesting.\\n\\nDeep flattening normalizes hierarchical data structures: collapsing nested DOM query results, processing recursive API responses, flattening multi-level category trees, and preparing deeply structured JSON for tabular display or database insertion.`,

  'js-rotate-left': `Left rotation shifts all elements toward the beginning by k positions, wrapping elements that fall off the left end back to the right. The slice-and-concatenate approach (arr.slice(k).concat(arr.slice(0, k))) achieves this elegantly in O(n).\\n\\nThe key insight is that rotation splits the array at position k and swaps the two halves. Handling k > arr.length requires modular arithmetic (k % arr.length) to normalize the rotation amount.\\n\\nArray rotation appears in cyclic buffer implementations, circular queue operations, string rotation problems (checking if one string is a rotation of another), and cryptographic ciphers. The three-reversal algorithm provides an in-place O(n) alternative.`,

  'js-interleave': `Interleaving merges two arrays by alternating elements: take one from the first, one from the second, and so on. When arrays differ in length, remaining elements from the longer array are appended.\\n\\nThe algorithm iterates up to the length of the longer array, pushing from each array if elements remain. Time complexity is O(m + n) where m and n are the array lengths.\\n\\nInterleaving is used in merge operations, round-robin scheduling, creating alternating visual patterns, combining audio channels, riffle shuffle simulation, and constructing test data that alternates between different types of values.`,

  'js-binary-search-iterative': `Iterative binary search uses a while loop with lo/hi pointers instead of recursion, avoiding stack overhead. The loop halves the search space each iteration by comparing the middle element to the target.\\n\\nThe critical implementation detail is using >>> 1 for the midpoint calculation to avoid integer overflow (though JavaScript uses floating-point, this is still good practice). Time complexity is O(log n) and space is O(1) since no recursion stack is used.\\n\\nIterative binary search is preferred in production code because it avoids potential stack overflow on huge datasets and has slightly less overhead. It is the form used in standard library implementations and is the version interviewers expect you to write fluently.`,

  'js-binary-search-insert': `Binary search for insertion point finds where a target should be placed to keep the array sorted. This is equivalent to the lower bound (bisect_left) operation, returning the first index where arr[i] >= target.\\n\\nThe key difference from standard binary search is the boundary handling: when the target is not found, lo converges to the correct insertion index rather than returning -1. Time complexity remains O(log n) with O(1) space.\\n\\nInsertion point search is used in maintaining sorted arrays, implementing sorted containers, computing rank statistics, and is the building block for operations like counting elements in a range or finding the floor/ceiling of a value.`,

  'js-merge-sorted': `Merging two sorted arrays uses a two-pointer technique: compare elements at the front of each array, take the smaller one, and advance that pointer. After one array is exhausted, append the remainder of the other.\\n\\nThis runs in O(m + n) time and O(m + n) space for the merged result. The comparison-based merge maintains stability (equal elements preserve their original order) and is the core subroutine of merge sort.\\n\\nSorted array merging is fundamental to merge sort (dividing and reconquering), database merge joins, combining sorted search results from multiple sources, k-way merge for external sorting, and maintaining sorted event timelines in simulation systems.`,

  'js-queue-operations': `Queue operations implement FIFO (First-In, First-Out) ordering where enqueue adds to the back and dequeue removes from the front. This is the natural ordering for processing items in arrival sequence.\\n\\nUsing push for enqueue and shift for dequeue on JavaScript arrays is simple but shift is O(n) due to re-indexing. For production use, a linked-list queue or circular buffer gives O(1) dequeue. Space complexity is O(n) for stored elements.\\n\\nQueues are fundamental to BFS traversal, task scheduling (print queues, job queues), message passing systems (RabbitMQ, SQS), the JavaScript event loop microtask queue, and any scenario requiring fair, ordered processing of arriving items.`,

  'js-min-heap-insert': `Min heap insertion adds a value to the end of the array and "bubbles it up" by repeatedly swapping with its parent while it is smaller. The parent of index i is at Math.floor((i-1)/2).\\n\\nInsertion runs in O(log n) time because the bubble-up traverses at most the height of the heap, which is floor(log2(n)). The heap is stored as a flat array where children of index i are at 2i+1 and 2i+2.\\n\\nMin heaps power priority queues, which are essential for Dijkstra's shortest path algorithm, Huffman encoding, event-driven simulation, top-K element queries, and process scheduling. Understanding the bubble-up and bubble-down operations is key to mastering heap-based algorithms.`,

  'js-graph-adjacency': `Converting an edge list to an adjacency list is the standard first step in graph algorithms. Each node maps to an array of its neighbors, enabling O(degree) neighbor iteration instead of O(E) edge scanning.\\n\\nFor undirected graphs, each edge [u,v] creates entries in both graph[u] and graph[v]. The nullish coalescing assignment (??=) initializes empty arrays on first access. Construction is O(V + E) time and space.\\n\\nAdjacency lists are the preferred representation for sparse graphs (most real-world graphs). They are the input format for BFS, DFS, Dijkstra, topological sort, and virtually every graph algorithm. Understanding this representation is a prerequisite for graph problem solving.`,

  'js-bfs-traversal': `Graph BFS explores nodes level by level outward from a starting node, using a queue and a visited set to avoid revisiting nodes. This guarantees that each node is discovered at its minimum distance from the source.\\n\\nTime complexity is O(V + E) since each vertex is enqueued once and each edge is examined once. Space is O(V) for the visited set and queue. Adding to visited before enqueuing (rather than when dequeuing) is critical to avoid duplicate queue entries.\\n\\nGraph BFS finds shortest paths in unweighted graphs, computes connected components, solves puzzle states (like sliding puzzles), powers social network friend-of-friend queries, and implements web crawlers that explore pages by link distance.`,

  'js-dfs-traversal': `Graph DFS explores as deeply as possible along each branch before backtracking, using either recursion or an explicit stack with a visited set. This depth-first exploration naturally discovers all reachable nodes.\\n\\nTime complexity is O(V + E) and space is O(V) for the visited set plus O(V) for the stack/recursion depth in the worst case. DFS explores a different order than BFS, prioritizing depth over breadth.\\n\\nDFS is used for cycle detection, topological sorting, finding connected/strongly-connected components, maze solving, and as the foundation for backtracking algorithms. In many graph problems, DFS is simpler to implement than BFS and uses less memory on narrow, deep graphs.`,

  'js-trie-insert': `Trie insertion builds a character-by-character tree structure where each node represents a prefix. Walking from the root to any node spells out the prefix formed by the edge labels along the path.\\n\\nInserting a word of length L takes O(L) time by creating child nodes for each character as needed and marking the terminal node with a sentinel ($ = true). Space depends on the total character count across all words, with shared prefixes saving space.\\n\\nTries power autocomplete systems, spell checkers, IP routing tables (longest prefix match), dictionary lookups in word games, and T9 keyboard prediction. The prefix-sharing property makes them far more space-efficient than storing all strings independently when there is significant overlap.`,

  'js-union-find': `Union-Find (Disjoint Set Union) tracks connected components with two operations: find (which component does x belong to?) and union (merge two components). Path compression makes find nearly O(1) amortized.\\n\\nPath compression flattens the tree during find by pointing every node directly to the root. Without rank optimization, this alone achieves O(log n) amortized per operation. Combined with union by rank, it achieves O(alpha(n)), the inverse Ackermann function, which is effectively constant.\\n\\nUnion-Find is used in Kruskal's minimum spanning tree algorithm, dynamic graph connectivity, image segmentation (connected pixel regions), social network component tracking, and detecting cycles in undirected graphs.`,

  'js-compose-functions': `Function composition chains multiple functions right-to-left so that compose(f, g, h)(x) computes f(g(h(x))). Using reduceRight processes functions from the last to the first, building the computation pipeline.\\n\\nComposition runs each function once, so time complexity is O(k) where k is the number of functions. The key insight is that reduceRight applies the innermost function first, matching the mathematical notation f . g . h.\\n\\nComposition is a cornerstone of functional programming, enabling point-free style and reusable transformation pipelines. It is used in Redux middleware chains, Express.js middleware stacking, data transformation pipelines, and anywhere you want to build complex behavior from simple, composable units.`,

  'js-pipe-functions': `Pipe is left-to-right function composition: pipe(f, g, h)(x) computes h(g(f(x))). Using reduce processes functions in the order they are listed, which many developers find more readable than right-to-left compose.\\n\\nLike compose, pipe has O(k) time complexity for k functions. The difference is purely directional: reduce applies functions left-to-right while reduceRight applies them right-to-left. The two are interchangeable by reversing the function list.\\n\\nPipe is popular in data processing (RxJS pipe operator, Node streams), build tool chains (gulp), command-line piping (conceptually), and is the basis for the proposed pipeline operator (|>) in JavaScript. It makes data flow explicit and sequential.`,

  'js-curry-function': `Currying transforms a multi-argument function into a chain of single-argument functions, so f(a, b, c) becomes f(a)(b)(c). The curried function collects arguments until enough are provided, then calls the original.\\n\\nThe recursive implementation returns a new function if fewer arguments than the function's arity have been collected, or calls the original function when all arguments are present. Partial application falls out naturally: curry(f)(a) returns a specialized function awaiting the remaining arguments.\\n\\nCurrying enables elegant partial application, function composition with fixed parameters, creating specialized validators from generic ones, and point-free programming. It is foundational in functional programming languages and widely used in React for event handler factories.`,

  'js-unique-by': `UniqueBy removes duplicates based on a key function rather than strict equality, keeping the first occurrence of each key. A Set tracks seen keys and filters out subsequent elements with the same key.\\n\\nThe algorithm runs in O(n) time with O(k) space for the Set where k is the number of unique keys. Using a Set for seen-key tracking gives O(1) lookup per element.\\n\\nUniqueBy is essential when deduplicating objects: removing duplicate users by ID, keeping the first product per category, deduplicating search results by URL, and any scenario where identity is determined by a derived key rather than the entire value.`,

  'js-difference': `Array difference returns elements present in the first array but not in the second, using a Set for efficient exclusion lookup. This implements the set-theoretic difference operation A - B.\\n\\nCreating the Set from the second array takes O(m) time, and filtering the first array takes O(n) time with O(1) per lookup, giving O(n + m) overall. Space is O(m) for the exclusion Set.\\n\\nDifference is used for finding new items (what was added since last sync), computing changelists, identifying missing elements, filtering blacklisted values, and implementing set operations for data reconciliation between systems.`,

  'js-intersection': `Array intersection returns elements present in both arrays using a Set for efficient membership testing. This implements the set-theoretic intersection A AND B.\\n\\nBuilding the Set from one array and filtering the other gives O(n + m) time. For optimal performance, build the Set from the smaller array to minimize space usage.\\n\\nIntersection finds common elements between datasets: mutual friends in social networks, common tags between articles, shared features between products, overlapping time ranges, and is used in search engines to combine results from multiple index lookups (conjunctive queries).`,

  'js-take-while': `TakeWhile collects elements from the start of an array while a predicate holds true, stopping at the first failure. Using findIndex to locate the first failing element, then slicing up to that point, gives a clean implementation.\\n\\nThe findIndex scan runs in O(n) worst case, and the slice is O(k) where k is the number of taken elements. If all elements pass, findIndex returns -1 and the entire array is returned.\\n\\nTakeWhile is a lazy evaluation primitive from functional programming. It is useful for reading the valid prefix of sorted data, processing log entries until a break condition, consuming tokens until a delimiter, and implementing early termination in stream processing.`,

  'js-drop-while': `DropWhile skips elements from the start while a predicate holds, returning everything from the first failure onward. It is the complement of takeWhile: dropWhile(p) + takeWhile(p) reconstructs the original array.\\n\\nLike takeWhile, it uses findIndex to locate the transition point, then slices from that index to the end. Time complexity is O(n) for the scan plus O(n-k) for the slice.\\n\\nDropWhile is used for skipping headers in data files, ignoring leading whitespace or noise, finding where interesting data begins in a sorted stream, and implementing iterator protocols that need to fast-forward past a known prefix.`,

  'js-sample-array': `Random sampling without replacement selects n distinct elements using a partial Fisher-Yates shuffle. For each position 0 to n-1, a random element from the unprocessed portion is swapped into place.\\n\\nThe partial shuffle runs in O(n) time (not O(arr.length)), making it efficient even for selecting a few elements from a large array. It uses O(arr.length) space for the copy to avoid mutating the input.\\n\\nFisher-Yates sampling is used in A/B test user assignment, Monte Carlo simulation, randomized algorithms (QuickSort pivot selection), creating random training/test splits in machine learning, and card dealing in games.`,

  'js-compact': `Compact removes all falsy values from an array using filter(Boolean). JavaScript has six falsy values: false, 0, "" (empty string), null, undefined, and NaN. Boolean as a filter predicate coerces each value and keeps only truthy ones.\\n\\nThis elegant one-liner runs in O(n) time. The Boolean constructor, when called as a function, performs the same type coercion as an if statement, making it a perfect predicate for filter.\\n\\nCompact is a data-cleaning utility used for stripping empty form fields before submission, removing null entries from parsed CSV data, cleaning up optional values from API responses, and preparing arrays for display where empty slots should be invisible.`,

  'js-count-by': `CountBy groups and counts elements using a custom key function, producing a histogram keyed by the function's return value. It extends basic frequency counting with the flexibility to count by any derived property.\\n\\nThe reduce implementation runs in O(n) time, applying the key function to each element and incrementing the corresponding count. Space is O(k) for k unique keys.\\n\\nCountBy powers analytics dashboards: counting users by country, orders by status, errors by type, and any categorical aggregation. It is the counting variant of groupBy and corresponds to SQL's SELECT key, COUNT(*) GROUP BY key pattern.`,

  'js-sum-by': `SumBy aggregates numeric values extracted from array elements by a function, using reduce to accumulate the total. It abstracts the common pattern of summing a specific property across objects.\\n\\nThe reduce runs in O(n) time with O(1) space. The value function extracts the numeric field, and the accumulator sums them. Starting with 0 handles empty arrays correctly.\\n\\nSumBy is used for computing order totals from line items, calculating portfolio value from stock positions, aggregating scores across categories, and any scenario where you need to total a specific numeric property across a collection of objects.`,

  'js-max-by': `MaxBy finds the element with the highest value for a given key function, returning the original element rather than just the key value. Reduce without an initial value starts with the first element as the candidate.\\n\\nThe reduce comparison runs in O(n) time with O(1) space. Calling keyFn twice per comparison (on the current max and the candidate) can be optimized by caching, but for most use cases the simpler code is preferred.\\n\\nMaxBy is used for finding the top scorer, most expensive product, latest timestamp, highest-priority task, or any "best by criteria" query. It avoids the need to sort the entire array when you only need the single maximum element.`,

  'js-trie-search': `Trie search traverses a prefix tree character by character to determine if a complete word exists. After navigating to the terminal node, checking the $ marker distinguishes complete words from mere prefixes.\\n\\nSearch runs in O(L) time where L is the word length, with O(1) per character lookup (assuming a hash-map-like child structure). This is independent of how many words are stored in the trie.\\n\\nTrie search is the read operation that makes tries practical: it powers dictionary validation in spell checkers, word existence checks in Scrabble/Wordle solvers, exact-match lookup in autocomplete systems, and routing table longest-prefix matching in network routers.`,

  'js-topological-sort': `Topological sort orders vertices of a DAG so every edge u->v has u before v. Kahn's algorithm uses in-degree counting: start with nodes having no incoming edges, process them, and reduce in-degrees of their neighbors.\\n\\nTime complexity is O(V + E) since each node is enqueued once and each edge is processed once. If the result has fewer than V nodes, the graph contains a cycle.\\n\\nTopological sort is essential for dependency resolution: build systems (make, webpack), course prerequisite ordering, package manager installation order, spreadsheet cell recalculation, and task scheduling. It also detects cycles in directed graphs as a side effect.`,

  'js-two-pointer-palindrome': `The two-pointer palindrome check converges pointers from both ends of the array, comparing elements as they move inward. If all pairs match, the array reads the same forwards and backwards.\\n\\nThis runs in O(n/2) = O(n) time with O(1) space, making it optimal for palindrome verification. Early termination on the first mismatch provides best-case O(1) performance.\\n\\nThe converging two-pointer technique extends far beyond palindromes. It solves two-sum on sorted arrays (move pointers based on sum comparison), container with most water, trapping rain water, and any problem where processing from both ends simultaneously provides useful information.`,

  'js-two-pointer-remove-dupes': `Removing duplicates from a sorted array uses slow/fast same-direction pointers. The slow pointer marks the write position while the fast pointer scans ahead, only writing when a new distinct value is found.\\n\\nThis achieves O(n) time with O(1) extra space by modifying the array in-place. The sorted property guarantees that duplicates are adjacent, so a single forward pass suffices.\\n\\nThe slow/fast pointer technique is a fundamental in-place modification pattern. It applies to removing specific values, compacting arrays, partitioning by a condition, and the classic "move zeroes to end" problem. It is one of the most commonly tested patterns in coding interviews.`,

  'js-sliding-window-max-sum': `The fixed-size sliding window finds the maximum sum of k consecutive elements by maintaining a running sum. After computing the initial window, each step adds the entering element and subtracts the leaving element.\\n\\nThis O(n) approach avoids the naive O(n*k) solution of recomputing each window sum from scratch. The sliding update windowSum += arr[i] - arr[i-k] is the key optimization.\\n\\nFixed-size sliding windows appear in moving average calculations, maximum/minimum over rolling periods, network throughput monitoring, finding the best k-day stock performance, and as the basis for more complex variable-size window problems.`,

  'js-sliding-window-min-subarray': `The variable-size sliding window finds the shortest subarray with sum >= target by expanding the right end and shrinking the left end when the condition is met. This expand-then-shrink pattern achieves O(n) time.\\n\\nThe key insight is that the left pointer only moves forward, so despite the nested while loop, each element is added and removed at most once. This amortized analysis proves the O(n) bound. Space is O(1).\\n\\nVariable-size windows solve minimum-length substring problems, bandwidth allocation windows, smallest range covering elements from multiple lists, and any optimization problem where you need the shortest contiguous segment satisfying a constraint.`,

  'js-prefix-sum': `A prefix sum array stores cumulative sums where prefix[i] = sum of arr[0..i]. This O(n) preprocessing enables O(1) range sum queries: sum(i,j) = prefix[j] - prefix[i-1].\\n\\nBuilding the prefix array is a single pass: each element is the previous prefix plus the current value. The elegance lies in turning any range sum query from O(k) (summing k elements) to O(1) (one subtraction).\\n\\nPrefix sums are used in subarray sum problems (finding subarrays summing to a target), image processing (summed area tables for fast box filters), competitive programming, equilibrium index problems, and any scenario requiring repeated range sum queries on static data.`,

  'js-prefix-product': `Product Except Self builds an output array where each element is the product of all other elements, without using division. Two passes handle this: a left-to-right pass computes prefix products, and a right-to-left pass multiplies in suffix products.\\n\\nThe two-pass approach runs in O(n) time and O(n) space (for the output). The no-division constraint avoids issues with zero elements and demonstrates the power of prefix/suffix decomposition.\\n\\nThis problem is a popular interview question that tests creative thinking beyond the obvious division approach. The prefix/suffix decomposition pattern applies broadly: computing prefix GCDs, suffix maximums, and any operation where you need "aggregate of everything except the current element."`,

  'js-difference-array': `A difference array enables O(1) range updates: to add a value v to all elements in range [l, r], set diff[l] += v and diff[r+1] -= v. Reconstructing the array requires a prefix sum pass over the difference array.\\n\\nMultiple range updates cost O(1) each, and the final reconstruction is O(n). This beats the naive O(n) per update when many updates are applied before reading. Total time is O(updates + n).\\n\\nDifference arrays are used in competitive programming for bulk range updates, booking systems (add capacity over time ranges), genomics (marking regions of interest), and flight capacity problems where many overlapping reservations modify the same availability array.`,

  'js-kadanes-algorithm': `Kadane's algorithm finds the maximum sum contiguous subarray in O(n) time by maintaining a running sum that resets when it goes negative. At each position, you either extend the current subarray or start fresh.\\n\\nThe key insight is: if the running sum becomes negative, no future extension can benefit from it, so starting over is always optimal. Track the global maximum across all positions. Space is O(1).\\n\\nKadane's is one of the most elegant dynamic programming algorithms. It appears in stock trading (maximum profit), signal processing (strongest signal segment), and is the foundation for 2D maximum subarray problems. It is also one of the most frequently asked easy/medium interview questions.`,

  'js-dutch-national-flag': `The Dutch National Flag algorithm partitions an array into three sections around a pivot value using three pointers: low, mid, and high. Elements less than the pivot go left, equal stay in the middle, and greater go right.\\n\\nThe algorithm runs in O(n) time with O(1) space, making a single pass. The mid pointer scans forward while low and high converge from the edges. This three-way partition is more efficient than two separate passes.\\n\\nThis algorithm is used in quicksort optimization (handling duplicates efficiently), color sorting, database query optimization for three-valued predicates, and any classification problem with exactly three categories. It was named by Dijkstra after the three colors of the Dutch flag.`,

  'js-fast-slow-pointers': `The fast-slow pointer (tortoise and hare) technique uses two pointers moving at different speeds to detect cycles in sequences. If a cycle exists, the fast pointer will eventually lap the slow pointer.\\n\\nDetection runs in O(n) time and O(1) space, avoiding the need for a visited set. Finding the cycle start requires a second phase where one pointer returns to the head and both advance at the same speed until they meet.\\n\\nThis technique detects cycles in linked lists, finds the start of the cycle, determines the cycle length, and applies to any sequence where a function maps elements to elements (like finding duplicate numbers). Floyd's algorithm is the classic application.`,

  'js-merge-in-place': `In-place merging of two sorted arrays without extra space uses the technique of placing elements from the end of the destination array. Starting from the last positions of both arrays, compare and place the larger element at the back.\\n\\nThis approach runs in O(m + n) time with O(1) extra space (assuming the first array has enough capacity). Working backwards prevents overwriting unprocessed elements.\\n\\nIn-place merge is used in merge sort optimization (avoiding auxiliary arrays), combining sorted database results with limited memory, and embedded systems where memory allocation is restricted. Understanding backwards merging is key to efficient in-place algorithms.`,

  'js-zigzag-iteration': `Zigzag (snake) traversal reads a matrix row by row, alternating direction each row: left-to-right for even rows, right-to-left for odd rows. This produces a continuous path that snakes through the grid.\\n\\nThe algorithm runs in O(m * n) time visiting each element once. A boolean flag or row-index parity determines the direction, and the row is reversed (or iterated backwards) for odd rows.\\n\\nZigzag traversal is used in image processing (some compression schemes like JPEG use zigzag scanning of DCT coefficients), CNC machine path planning (minimizing direction changes), and printing patterns in matrix-related interview questions.`,

  'js-spiral-matrix': `Spiral traversal reads a matrix in a clockwise spiral: right across the top, down the right side, left across the bottom, up the left side, then inward. Four boundary variables (top, bottom, left, right) track the shrinking perimeter.\\n\\nTime complexity is O(m * n) since every element is visited once. The four boundaries are adjusted inward after each direction is completed. Careful boundary checks prevent revisiting or out-of-bounds access.\\n\\nSpiral order is a classic matrix interview problem that tests careful boundary management. It appears in matrix rotation problems, space-filling curve generation, and as a component of more complex matrix manipulation algorithms.`,

  'js-diagonal-traversal': `Diagonal traversal visits matrix elements along anti-diagonals (top-right to bottom-left). For an m*n matrix, there are m+n-1 diagonals, each defined by a constant sum of row and column indices.\\n\\nThe algorithm iterates over diagonals 0 to m+n-2, computing the starting position for each diagonal and walking diagonally. Time complexity is O(m * n) with O(1) extra space.\\n\\nDiagonal traversal appears in image processing (diagonal filters), chess and game board analysis (diagonal threats), JPEG coefficient ordering, and matrix problems requiring non-standard access patterns.`,

  'js-rotate-matrix': `In-place matrix rotation by 90 degrees clockwise uses a two-step approach: first transpose (swap rows and columns), then reverse each row. This elegantly achieves the rotation without extra space.\\n\\nBoth steps run in O(n^2) for an n*n matrix, giving O(n^2) total. The transpose swaps matrix[i][j] with matrix[j][i] for j > i (upper triangle), and row reversal is O(n) per row.\\n\\nMatrix rotation is a classic interview problem that tests in-place transformation skills. It is used in image rotation, game board manipulation (Tetris piece rotation), and computer graphics. Understanding that rotation = transpose + reverse is a powerful geometric insight.`,

  'js-lower-bound': `Lower bound finds the first index where arr[i] >= target using binary search with lo/hi pointers. When arr[mid] < target, search right (lo = mid+1); otherwise search left (hi = mid). The answer is lo when the loop exits.\\n\\nThis runs in O(log n) time and O(1) space. The boundary at hi = arr.length (not arr.length - 1) allows the result to indicate "target is larger than all elements."\\n\\nLower bound is the most versatile binary search variant. Combined with upper bound, it counts occurrences, finds ranges, and implements multiset operations. It is the foundation of C++ lower_bound, Python bisect_left, and is the binary search form most commonly needed in interviews.`,

  'js-upper-bound': `Upper bound finds the first index where arr[i] > target (strictly greater), complementing lower bound. The only difference is using <= instead of < in the comparison, which pushes the boundary past equal elements.\\n\\nLike lower bound, it runs in O(log n) time and O(1) space. The pair of lower and upper bound together defines the range [lower, upper) of elements equal to target.\\n\\nUpper bound combined with lower bound enables O(log n) range queries on sorted arrays: counting occurrences (upper - lower), checking existence, and finding the last occurrence (upper - 1). This pair is the building block for sorted container operations.`,

  'js-binary-search-sqrt': `Integer square root via binary search demonstrates the "binary search on the answer" meta-technique. Instead of searching an array, you search over possible answer values [0, n] for the largest x where x*x <= n.\\n\\nThe search space halves each iteration, giving O(log n) time and O(1) space. The termination condition returns hi (the right pointer), which is the last value satisfying x*x <= n after the loop converges.\\n\\nBinary search on the answer is a powerful paradigm that applies whenever you can verify a candidate answer efficiently. It solves optimization problems like minimizing maximum allocation, finding capacity thresholds, and computing roots of monotonic functions.`,

  'js-search-rotated': `Searching a rotated sorted array requires identifying which half is properly sorted and checking if the target falls within that sorted range. At each step, one half is guaranteed to be sorted, enabling binary search decisions.\\n\\nThe algorithm maintains O(log n) time by eliminating half the search space each iteration. The key insight is comparing arr[lo] with arr[mid]: if arr[lo] <= arr[mid], the left half is sorted; otherwise the right half is sorted.\\n\\nThis is one of the most frequently asked binary search interview problems. It tests the ability to reason about partially sorted data and make correct binary decisions in non-standard search scenarios. Variations include finding the minimum and handling duplicates.`,

  'js-quick-select': `QuickSelect finds the kth smallest element in O(n) average time using the partition subroutine from quicksort. After partitioning, the pivot is in its final sorted position; recurse only on the side containing the target index.\\n\\nAverage time is O(n) because each partition step processes a geometrically shrinking subarray. Worst case is O(n^2) with adversarial pivots, mitigated by random pivot selection or median-of-medians.\\n\\nQuickSelect is the standard algorithm for order statistics: finding medians, percentiles, and top-K elements without full sorting. It is used in database query optimization, statistical analysis, and is the algorithm behind NumPy's partition function.`,

  'js-exponential-search': `Exponential search finds an element by first doubling a bound (1, 2, 4, 8, ...) until it exceeds the target, then binary searching within the found range. This combines O(log n) search with unbounded range finding.\\n\\nThe doubling phase takes O(log i) where i is the target's position, and the binary search within [bound/2, bound] takes O(log i) as well. Total time is O(log i), which is optimal when the target is near the beginning of a very large or unbounded array.\\n\\nExponential search is ideal for unbounded or infinite sorted sequences (like searching in an infinitely scrolling list), and when elements near the beginning are more likely targets. It also teaches the doubling technique used in dynamic array resizing.`,

  'js-find-peak': `Peak finding uses binary search on a non-sorted array by always moving toward the higher neighbor. If arr[mid] < arr[mid+1], a peak must exist to the right; otherwise it exists at mid or to the left.\\n\\nThis converges in O(log n) time because each step eliminates half the search space. The correctness relies on the boundary assumption that arr[-1] = arr[n] = -infinity, guaranteeing at least one peak exists.\\n\\nPeak finding demonstrates that binary search applies beyond sorted data, requiring only a "gradient" property. It is used in finding local optima, bitonic search, and optimization problems where you can determine which direction improves the objective function.`,

  'js-search-2d-matrix': `Searching a row-and-column-sorted matrix from the top-right corner eliminates one row or column per step. If the current value is too large, move left; if too small, move down. This staircase path reaches the target or exhausts the matrix.\\n\\nTime complexity is O(m + n) since each step moves left or down, and you can make at most m + n such moves. Space is O(1). This is optimal for matrices sorted in both dimensions.\\n\\nThis elegant elimination technique is a popular interview problem. It applies to searching any structure where one axis is ascending and the other provides a second ordering, and teaches the concept of exploiting multiple sorted dimensions simultaneously.`,

  'js-count-occurrences': `Counting occurrences in a sorted array uses two binary searches: lower bound (first index >= target) and upper bound (first index > target). The count is simply upper - lower, running in O(log n).\\n\\nThis is far more efficient than linear scanning for large arrays. The technique generalizes: any range query on a sorted array can be answered with two binary searches.\\n\\nThis pattern is used in database range queries, counting elements in a range, implementing multiset size queries, and is a common interview follow-up to basic binary search. It demonstrates the power of combining lower and upper bound as a pair.`,

  'js-min-in-rotated': `Finding the minimum in a rotated sorted array uses binary search by comparing the mid element with the right boundary. If arr[mid] > arr[hi], the minimum is in the right half; otherwise it is in the left half including mid.\\n\\nThis runs in O(log n) time and O(1) space. The key insight is that the minimum is the rotation point where the sorted order breaks, and comparing with arr[hi] determines which side the break is on.\\n\\nThis problem frequently appears as an interview variant of rotated array problems. It teaches boundary analysis in binary search and is a stepping stone to the harder "search in rotated sorted array" problem.`,

  'js-max-heap-insert': `Max heap insertion mirrors min heap insertion but maintains the opposite invariant: every parent is greater than or equal to its children. The bubble-up swaps with the parent while the inserted value is larger.\\n\\nLike min heap, insertion is O(log n) time. The only difference is the comparison direction: swap when heap[parent] < heap[i] instead of heap[parent] > heap[i].\\n\\nMax heaps are used for extracting the largest element efficiently, implementing max-priority queues, finding the kth largest element, heap sort (which uses a max heap to sort in ascending order), and scheduling the highest-priority task first.`,

  'js-heap-extract-min': `Extracting the minimum from a min heap removes the root (smallest element), moves the last element to the root, and "bubbles it down" by swapping with the smaller child until the heap property is restored.\\n\\nExtraction runs in O(log n) time. The bubble-down compares with both children at each level, choosing the smaller one to swap with. This maintains the min-heap invariant throughout the tree.\\n\\nHeap extraction is the core operation powering priority queue polling. It is used in every iteration of Dijkstra's algorithm, Huffman tree construction, event simulation (process the earliest event), and any algorithm that repeatedly needs the minimum remaining element.`,

  'js-lru-cache': `An LRU Cache evicts the least recently used entry when capacity is reached. JavaScript's Map preserves insertion order, so re-inserting a key (delete then set) moves it to the most-recent position, and the first key is the least recently used.\\n\\nAll operations (get, put) run in O(1) time using Map's O(1) get/set/delete. The eviction uses Map.keys().next().value to find the oldest entry. Space is O(capacity).\\n\\nLRU caches are everywhere: browser HTTP caches, database query caches, operating system page replacement, CDN edge caching, and in-memory application caches. Understanding LRU is essential for system design interviews and practical performance optimization.`,

  'js-linked-list-reverse': `Linked list reversal rewires the next pointers in-place using three variables: prev, current, and next. At each step, save current.next, point current.next to prev, then advance both prev and current.\\n\\nReversal runs in O(n) time with O(1) extra space. The three-pointer technique is the canonical pattern for in-place linked list manipulation. The key is saving the next pointer before overwriting it.\\n\\nList reversal is the most commonly asked linked list question in interviews. It is a building block for checking if a list is a palindrome (reverse the second half), reversing in groups of k, and any problem requiring backward traversal of a singly-linked list.`,

  'js-circular-buffer': `A circular buffer uses a fixed-size array with head and tail pointers that wrap around using modular arithmetic: (index + 1) % capacity. A separate count variable distinguishes full from empty states.\\n\\nAll operations (enqueue, dequeue, peek) run in O(1) time. The modular wrapping reuses array slots without shifting elements, unlike a standard array queue where dequeue requires O(n) shifting.\\n\\nCircular buffers are used in audio/video streaming (bounded playback buffers), network packet buffering, logging systems (overwrite oldest entries), and producer-consumer queues in operating systems. They provide predictable memory usage with constant-time operations.`,

  'js-monotonic-stack': `A monotonic stack maintains elements in decreasing order and efficiently finds the next greater element for each array element. When a new element is larger than the stack top, all smaller stack elements have found their answer.\\n\\nDespite the nested loop appearance, each element is pushed and popped at most once, giving O(n) total time. Space is O(n) for the stack and result array.\\n\\nThe monotonic stack pattern solves a family of problems: next greater/smaller element, stock span, largest rectangle in histogram, trapping rain water, and temperature wait-time questions. Recognizing when this pattern applies is a valuable interview skill.`,

  'js-sliding-window-max': `Sliding window maximum uses a monotonic deque (double-ended queue) that maintains indices in decreasing order of their values. The front always holds the index of the current window maximum.\\n\\nEach element is added and removed from the deque at most once, giving O(n) total time regardless of window size k. The deque is pruned from the front (expired indices) and the back (smaller values).\\n\\nThis combines two patterns (sliding window + monotonic data structure) and is considered an advanced technique. It is used for real-time maximum tracking in data streams, stock price analysis, and solving problems like "minimum of maximums over all windows."`,

  'js-min-stack': `Min Stack supports push, pop, top, and getMin all in O(1) by maintaining an auxiliary stack that tracks the current minimum. When pushing a value <= the current minimum, push it onto the min stack too. When popping that value, pop from the min stack as well.\\n\\nBoth stacks grow to at most n elements, giving O(n) space. The key insight is that the min stack only needs entries when the minimum changes, so it often remains much smaller than the main stack.\\n\\nMin stack is a classic design problem that teaches maintaining parallel state for aggregate queries. The same technique extends to max stack, and the concept generalizes to augmented data structures that track additional statistics alongside their primary data.`,

  'js-two-stack-queue': `A queue built from two stacks achieves amortized O(1) per operation through lazy transfer. Enqueue pushes onto the input stack; dequeue pops from the output stack. When the output stack is empty, all input stack elements are transferred in one batch.\\n\\nEach element is moved at most twice (once to input, once to output), so n operations take O(n) total time, giving O(1) amortized per operation. This is a classic example of amortized analysis.\\n\\nThis design problem appears frequently in interviews and teaches that combining simple data structures can create new ones with different properties. It is also how some functional programming languages implement efficient persistent queues.`,

  'js-disjoint-set-rank': `Union-Find with both rank and path compression achieves near-constant-time operations. Path compression flattens the tree during find (every node points directly to the root). Union by rank always attaches the shorter tree under the taller one.\\n\\nThe combined optimizations give O(alpha(n)) amortized per operation, where alpha is the inverse Ackermann function, which is at most 4 for any practical input size. This is essentially O(1).\\n\\nThis optimized DSU is used in Kruskal's minimum spanning tree algorithm, dynamic connectivity in networks, image component labeling, equivalence class merging, and percolation simulation. Understanding both optimizations together is important for competitive programming.`,

  'js-weighted-graph': `Building a weighted adjacency list from [u, v, weight] triples stores {node, weight} objects for each neighbor. For undirected graphs, edges are added in both directions. Pre-initializing all node arrays ensures every node appears in the graph.\\n\\nConstruction runs in O(V + E) time and space. The weighted format supports shortest-path algorithms that need edge weights, unlike unweighted adjacency lists that store only neighbor IDs.\\n\\nWeighted adjacency lists are the standard input for Dijkstra's algorithm, Bellman-Ford, Floyd-Warshall, Prim's MST, and A* search. Most real-world graphs are weighted (distances, costs, latencies), making this the most practical graph representation.`,

  'js-fenwick-tree-update': `A Fenwick tree (Binary Indexed Tree) supports point updates and prefix sum queries in O(log n) each. Updates propagate upward through the tree by adding the lowest set bit to the index: i += i & (-i).\\n\\nThe tree uses a 1-indexed array where each position stores a partial sum covering a range determined by its lowest set bit. The update path visits O(log n) positions.\\n\\nFenwick trees are compact and cache-friendly alternatives to segment trees for problems involving prefix sums with updates. They power range sum queries in competitive programming, frequency counting in sorted streams, and inversion counting algorithms.`,

  'js-fenwick-tree-query': `Fenwick tree prefix sum queries accumulate partial sums by walking down through the tree, removing the lowest set bit at each step: i -= i & (-i). This visits O(log n) nodes to compute the prefix sum.\\n\\nThe query and update operations mirror each other: queries walk down (removing bits) while updates walk up (adding bits). Together they give O(log n) for both operations with only O(n) space.\\n\\nFenwick tree queries enable dynamic range sum computations where the underlying data changes frequently. Applications include real-time leaderboard ranking, dynamic frequency tables, and any scenario requiring fast cumulative statistics over a mutable array.`,

  'js-hash-map-chaining': `Hash map with chaining stores key-value pairs in buckets (arrays) indexed by hash(key) % capacity. Collisions are handled by appending to the bucket's chain, and lookups scan the chain linearly.\\n\\nAverage-case get/put/remove operations are O(1) when the load factor is low, degrading to O(n/buckets) as chains grow. A good hash function distributes keys uniformly across buckets.\\n\\nUnderstanding hash map internals is fundamental: it explains why JavaScript objects and Maps are O(1) average-case, why hash collisions cause performance degradation, and how hash tables power databases, caches, symbol tables in compilers, and virtually every associative data structure.`,

  'js-deque': `A deque (double-ended queue) supports push and pop from both ends: pushFront, pushBack, popFront, and popBack. Using a JavaScript array, unshift/shift handle the front and push/pop handle the back.\\n\\nWith native arrays, front operations are O(n) due to element shifting, while back operations are O(1). For O(1) all-around, a doubly-linked list or circular buffer implementation is needed.\\n\\nDeques are used in the sliding window maximum algorithm (monotonic deque), implementing both stacks and queues, BFS with 0-1 edge weights (0-1 BFS), palindrome checking, and work-stealing schedulers in parallel computing.`,

  'js-fast-power': `Binary exponentiation computes base^exp in O(log exp) time by exploiting the property that x^(2k) = (x^k)^2. For even exponents, square the half-result; for odd, multiply by base and reduce to even.\\n\\nThe recursion depth is O(log exp), with one multiplication per level. This is exponentially faster than naive repeated multiplication which takes O(exp) time.\\n\\nFast exponentiation is essential in modular arithmetic (computing a^b mod m for RSA cryptography), matrix exponentiation (computing Fibonacci numbers in O(log n)), competitive programming, and any computation involving large exponents.`,

  'js-flood-fill': `Flood fill colors all connected same-colored cells starting from a given position, using recursive DFS in four directions. It first checks bounds and color match, then recolors the current cell and recurses to all neighbors.\\n\\nTime complexity is O(m * n) in the worst case (filling the entire grid), with recursion depth also O(m * n). The recoloring itself serves as the visited marker, preventing revisits.\\n\\nFlood fill is the algorithm behind the paint bucket tool in image editors. It also forms the basis for island counting (how many connected regions), region labeling in image segmentation, solving maze reachability, and implementing go-game territory counting.`,

  'js-generate-parens': `Generating valid parentheses uses constraint-based backtracking: you can add "(" only if fewer than n opens have been placed, and ")" only if fewer closes than opens have been placed. These rules ensure every generated string is balanced.\\n\\nThe number of valid strings is the nth Catalan number C(n) = (2n)! / ((n+1)!n!). The backtracking tree is pruned by the constraints, exploring only valid paths. Time is O(4^n / sqrt(n)) per the Catalan bound.\\n\\nThis problem teaches constrained generation and is directly applicable to generating valid expressions, syntax trees, nested structure templates, and Catalan number enumeration. It is one of the most commonly asked medium-difficulty interview problems.`,

  'js-tower-of-hanoi': `Tower of Hanoi solves by recursion: move n-1 disks to auxiliary, move the largest disk to target, then move n-1 disks from auxiliary to target. This produces 2^n - 1 moves, which is provably optimal.\\n\\nEach recursive call reduces the problem size by 1, creating a binary recursion tree of depth n. The move sequence demonstrates how a complex problem decomposes into identical but smaller subproblems.\\n\\nBeyond being a classic recursion teaching tool, Hanoi appears in backup rotation schemes (Tower of Hanoi backup), understanding recursive algorithm complexity, and as a model for divide-and-conquer problem decomposition. The exponential growth in moves illustrates fundamental computational limits.`,

  'js-deep-clone': `Deep cloning recursively copies nested objects and arrays so the clone shares no references with the original. Type checking at each level determines whether to recurse (objects/arrays), directly return (primitives/null), or map (arrays).\\n\\nTime complexity is O(n) where n is the total number of values across all nesting levels. The recursion depth equals the maximum nesting depth. Note that this basic implementation does not handle circular references, dates, RegExp, Maps, or Sets.\\n\\nDeep cloning is essential for immutable state management in React/Redux (preventing accidental mutation of state), creating independent copies of configuration objects, snapshot-based undo systems, and safely passing complex data between modules.`,

  'js-subset-sum-exists': `Subset sum asks whether any combination of array elements adds up to a target. The backtracking approach tries including or excluding each element, pruning branches where the current sum exceeds the target.\\n\\nThe worst case is O(2^n) since all subsets might be explored. The early termination when currentSum > target (assuming positive integers) significantly prunes the search tree in practice.\\n\\nSubset sum is NP-complete, making it a foundational problem in computational complexity. It appears in budget allocation, bin packing, cryptographic knapsack systems, and is the basis for the 0/1 knapsack problem. The backtracking pattern here generalizes to all constraint satisfaction problems.`,

  'js-n-queens-count': `N-Queens places n non-attacking queens on an n*n board using backtracking with constraint propagation. Three Sets track occupied columns and both diagonal directions, enabling O(1) conflict checking per placement.\\n\\nThe algorithm places queens row by row, trying each column and pruning immediately when conflicts are detected. For n=8 there are 92 solutions. The time complexity is bounded by n! but pruning makes it much faster in practice.\\n\\nN-Queens is the canonical backtracking problem that teaches systematic state-space exploration with constraint propagation. The diagonal tracking technique (row-col and row+col) generalizes to any grid-based constraint, and the add-recurse-remove pattern is the template for all backtracking algorithms.`,

  'js-word-search-grid': `Word search combines grid DFS with backtracking, starting from every cell and extending in four directions while matching characters. Visited cells are marked with a sentinel to prevent reuse, then restored when backtracking.\\n\\nWorst case is O(m * n * 4^L) where L is the word length, since each cell can branch four ways for each character. The visited marking and character mismatch pruning make it much faster in practice.\\n\\nWord search is one of the most commonly asked medium-difficulty interview problems. It combines grid traversal, DFS, and backtracking in a single problem and is representative of the pattern used in puzzle solvers, path finding with constraints, and constraint satisfaction on grids.`,

  'js-flatten-nested-recursive': `Recursive deep flattening traverses arbitrarily nested arrays, pushing non-array elements to a result array and recursing into array elements. Array.isArray distinguishes the two cases.\\n\\nTime is O(n) where n is the total number of leaf values across all nesting levels. Stack depth equals the maximum nesting depth. This is simpler than depth-limited flattening since it always recurses fully.\\n\\nThis pattern is the basis for processing any tree-like structure stored as nested arrays: normalizing JSON API responses with variable nesting, processing recursive file system listings, flattening nested comment threads, and implementing Array.prototype.flat(Infinity).`,

  'js-string-perms-dedup': `Unique permutations of a string with duplicates uses sort-and-skip deduplication. After sorting characters so duplicates are adjacent, the rule "skip character i if it equals character i-1 and i-1 was not used" prevents generating the same permutation twice.\\n\\nThe sort is O(n log n) and the backtracking explores at most n!/product(ki!) unique permutations where ki are the character frequency counts. Without deduplication, duplicates would cause the same permutation to appear multiple times.\\n\\nThis deduplication technique applies to any combinatorial generation with repeated elements: generating unique combinations, partitions, and arrangements. It is a critical optimization skill for interview problems involving elements with duplicates.`,

  'js-combinations-with-rep': `Combinations with repetition allow elements to be chosen multiple times, modeled by recursing from the same index (i) instead of the next one (i+1). This produces multiset combinations in non-decreasing order.\\n\\nThe number of results is C(n+k-1, k) where n is the array size and k is the selection size. The backtracking pattern is identical to standard combinations except for the starting index of the recursive call.\\n\\nThis variant models real-world scenarios like making change with unlimited coins, selecting ice cream scoops (flavors can repeat), distributing identical resources to buckets, and is the combinatorial foundation for the stars-and-bars counting technique.`,

  'js-next-permutation': `Next lexicographic permutation transforms an array in-place using three steps: find the rightmost ascent (pivot), swap the pivot with the smallest larger element to its right, then reverse the suffix after the pivot position.\\n\\nThis O(n) algorithm advances to the next permutation without generating all prior ones. If no pivot exists (the array is fully descending), reversing the entire array wraps around to the smallest permutation.\\n\\nNext permutation enables iterating through all n! permutations in order using O(1) extra space per step. It is used in combinatorial search, test case enumeration, lexicographic ranking, and is the algorithm behind C++ std::next_permutation.`,

  'js-permutation-rank': `Permutation ranking computes the 1-based lexicographic position of a permutation by counting, for each position, how many unused smaller elements could have been placed there. Each such element accounts for (remaining positions)! earlier permutations.\\n\\nThe algorithm runs in O(n^2) time: for each of n positions, it counts smaller elements among the remaining ones. An optimized version using a Fenwick tree achieves O(n log n).\\n\\nPermutation ranking enables compact encoding of permutations as single integers, useful for puzzle state compression (Rubik's cube solvers), hashing permutation states, implementing the Lehmer code, and bijective mapping between integers and arrangements.`,

  'js-derangements-count': `Derangements are permutations where no element appears in its original position. The recurrence D(n) = (n-1) * (D(n-1) + D(n-2)) counts them efficiently in O(n) time with O(1) space using two rolling variables.\\n\\nThe recurrence has an elegant interpretation: for element 1, choose any of n-1 positions k. If element k goes to position 1 (swap), the rest is D(n-2). If element k does not go to position 1, it is a derangement of n-1 elements, giving D(n-1).\\n\\nDerangements appear in the hat-check problem (probability nobody gets their own hat), secret Santa assignment validation, counting fixed-point-free permutations in group theory, and approximating the probability of derangement (approaching 1/e as n grows).`,

  'js-pascals-triangle-row': `Pascal's triangle row generation builds iteratively: each row's interior elements are the sum of two adjacent elements from the previous row, with 1s on both ends. Row n contains the binomial coefficients C(n,0) through C(n,n).\\n\\nBuilding row n takes O(n) time per row and O(n) space. The iterative approach processes one row at a time, discarding previous rows, which is more memory-efficient than building the entire triangle.\\n\\nPascal's triangle encodes binomial coefficients used in probability (binomial distribution), polynomial expansion ((a+b)^n), combinatorial identities, and is closely related to the Sierpinski triangle fractal. It is a foundational structure connecting algebra, combinatorics, and number theory.`,

  'js-catalan-number': `Catalan numbers follow the recurrence C(n) = sum of C(i)*C(n-1-i) for i=0..n-1, with base case C(0) = 1. The DP table fills in O(n^2) time, computing each C(i) from previously computed values.\\n\\nThe recurrence reflects the structure of Catalan-counted objects: splitting into a left part of size i and a right part of size n-1-i, then multiplying the counts. This is the same decomposition used for BST counting and parenthesization.\\n\\nCatalan numbers count an astonishing variety of structures: balanced parentheses strings, distinct BSTs with n keys, ways to triangulate a polygon, mountain range profiles, non-crossing partitions, and Dyck paths. They grow as approximately 4^n / (n^(3/2) * sqrt(pi)).`,

  'js-power-set-bitmask': `Bitmask enumeration generates all subsets by iterating integers from 0 to 2^n - 1, where each bit position determines whether the corresponding array element is included. This replaces recursion with simple bit checking.\\n\\nTime complexity is O(2^n * n) since each of 2^n masks requires checking n bits. The approach is more cache-friendly and has less overhead than recursive subset generation, making it faster in practice for small n.\\n\\nBitmask enumeration is widely used in competitive programming for state compression DP, exhaustive search over subsets, representing sets as integers for hashing, and implementing game AI that explores all possible move combinations.`,

  'js-gray-code': `Gray code generates a sequence where consecutive values differ by exactly one bit, using the formula gray(i) = i XOR (i >> 1). This produces a Hamiltonian path through the binary hypercube.\\n\\nGeneration runs in O(2^n) time with O(1) per value. The XOR formula is a closed-form expression that directly computes each Gray code value without needing the previous one.\\n\\nGray codes minimize signal errors in physical encoding systems (rotary encoders, analog-to-digital converters) because a single-bit transition cannot produce a large error. They are also used in Karnaugh maps for logic minimization, genetic algorithms, and generating subsets in an order where each differs minimally from the previous.`,

  'js-josephus': `The Josephus problem finds the survivor when people in a circle are eliminated every k-th position. The iterative formula J(i) = (J(i-1) + k) % i builds the answer from 1 person up to n, running in O(n) time and O(1) space.\\n\\nThe modular arithmetic elegantly handles the circular elimination. Starting with J(1) = 0 (the only person at position 0), each step maps the survivor position from a smaller circle to a larger one by shifting by k positions.\\n\\nThe Josephus problem has historical origins and appears in game theory, competitive programming, and as an example of mathematical induction applied to circular structures. The O(n) iterative solution is far more elegant than simulating the actual elimination process.`,

  'js-count-inversions': `Counting inversions via merge sort tallies "cross-inversions" during the merge step. When an element from the right half is smaller than elements remaining in the left half, it forms inversions with all of them at once.\\n\\nThe modified merge sort runs in O(n log n) time, matching the sorting bound. Each merge step counts inversions as a free byproduct of the merge comparison. The divide-and-conquer decomposition handles left-inversions, right-inversions, and cross-inversions separately.\\n\\nInversion count measures how far an array is from sorted (0 inversions = sorted, n(n-1)/2 = reverse sorted). It is used in computing Kendall tau rank correlation, analyzing sorting algorithm performance, recommendation systems, and as a difficulty metric for permutations.`,

  'js-preorder-iterative': `Iterative preorder uses an explicit stack to simulate the recursive call stack. Pop a node, visit it, then push right child before left child so that left is processed first (LIFO order ensures left pops before right).\\n\\nTime is O(n) and space is O(h) for the stack, identical to recursive DFS. The explicit stack avoids potential stack overflow on very deep trees and makes the traversal order manipulation more visible.\\n\\nConverting recursive algorithms to iterative form is a crucial skill for production code and interviews. Interviewers often ask for iterative tree traversals as follow-ups to recursive ones, testing understanding of how recursion maps to stack operations.`,

  'js-postorder-iterative': `Iterative postorder uses a clever trick: perform a modified preorder (root, right, left) and reverse the result to get (left, right, root). This avoids the complex state tracking that a direct iterative postorder requires.\\n\\nPush left before right (opposite of preorder) so the stack processes root-right-left. Reversing gives left-right-root, which is postorder. Time is O(n) and space is O(n) for the result array.\\n\\nIterative postorder is the trickiest of the three traversals because the root must be visited last. The reverse trick is elegant but uses O(n) extra space. A direct approach using a "last visited" pointer exists but is significantly more complex. This is a common advanced interview question.`,

  'js-zigzag-level-order': `Zigzag level order traversal uses BFS with a direction flag that alternates each level. Process nodes normally (left to right) using a queue, but reverse the level array for odd-numbered levels.\\n\\nTime is O(n) since each node is visited once, and the reversal of each level adds at most O(n) total across all levels. Space is O(w) for the queue where w is the maximum width.\\n\\nThis problem combines BFS level-by-level processing with direction toggling, testing the ability to adapt standard algorithms. It appears in tree visualization, binary tree printing, and as a variation of level-order traversal in interviews.`,

  'js-tree-level-widths': `Tree level widths computes the node count at each level using BFS with level-size tracking. Before processing each level, queue.length gives the width, which is recorded before dequeuing and enqueueing children.\\n\\nTime is O(n) with O(w) space for the queue. The level-size technique (processing exactly queue.length nodes per iteration) is the standard pattern for any BFS problem requiring level-by-level grouping.\\n\\nLevel width computation finds the maximum width of a tree, enables level-based aggregation (average value per level), detects whether a tree is complete, and is used in tree visualization layout algorithms.`,

  'js-lowest-common-ancestor': `LCA uses recursive divide-and-conquer: if the current node matches either target, return it. Otherwise, recurse on both subtrees. If both return non-null, the current node is the LCA; otherwise, return whichever side found something.\\n\\nTime is O(n) in the worst case (visiting all nodes) and O(h) space for the recursion stack. The algorithm works for any binary tree, not just BSTs.\\n\\nLCA is fundamental for computing distances between tree nodes, finding paths between nodes, and is used in phylogenetic analysis (finding common ancestors of species), version control systems (finding merge bases), and DOM traversal (finding the common container of two elements).`,

  'js-tree-diameter': `Tree diameter finds the longest path between any two nodes by computing heights recursively and tracking the maximum leftHeight + rightHeight across all nodes. The height function returns one value while a closure variable tracks the diameter.\\n\\nTime is O(n) with a single pass. The pattern of "returning one value while tracking another" is a common recursive technique. The diameter may not pass through the root.\\n\\nTree diameter is used in network analysis (furthest nodes in a network), computing the eccentricity of a graph, tree center finding (for balanced tree construction), and as a component of tree decomposition algorithms.`,

  'js-serialize-tree': `Tree serialization converts a binary tree to a string using preorder traversal with "null" markers for missing children. Deserialization reconstructs the tree by consuming tokens in the same preorder sequence.\\n\\nBoth serialize and deserialize run in O(n) time. The preorder format with null markers uniquely encodes any binary tree shape. The index-based token consumption during deserialization naturally handles the recursive structure.\\n\\nTree serialization is essential for persistent storage (saving trees to disk or database), network transmission of tree data (API responses), clipboard copy/paste of tree structures, and is a frequently asked system design interview question.`,

  'js-lcs-length': `LCS builds a 2D DP table where dp[i][j] is the LCS length of the first i characters of string a and first j characters of string b. Matching characters extend the diagonal, while mismatches take the maximum of skipping either character.\\n\\nTime and space are both O(m * n). The table can be optimized to O(min(m,n)) space by keeping only two rows, since each cell depends only on the current and previous row.\\n\\nLCS is the algorithm behind diff tools (git diff, unified diff), DNA sequence alignment (bioinformatics), spell checking suggestions, file comparison, and is the foundational two-string DP problem. Understanding LCS prepares you for edit distance, shortest common supersequence, and other string DP variants.`,

  'js-edit-distance': `Edit distance (Levenshtein) computes the minimum insertions, deletions, and substitutions to transform one string into another. The 2D DP table has base cases dp[i][0] = i and dp[0][j] = j, representing transforming to/from empty strings.\\n\\nTime and space are O(m * n). Each cell considers three operations: insert (dp[i][j-1] + 1), delete (dp[i-1][j] + 1), and replace (dp[i-1][j-1] + 1 if characters differ). If characters match, dp[i][j] = dp[i-1][j-1].\\n\\nEdit distance powers spell checkers, fuzzy search, DNA sequence alignment, plagiarism detection, and auto-correct systems. It is one of the most important string algorithms and a staple of technical interviews.`,

  'js-coin-change-min': `Coin change finds the minimum number of coins to make an amount using 1D DP. For each amount i from 1 to target, try every coin denomination: dp[i] = min(dp[i], dp[i-coin] + 1) if coin <= i.\\n\\nTime is O(amount * coins) and space is O(amount). The unbounded nature (each coin can be used multiple times) is reflected in the inner loop iterating over all coins for each amount.\\n\\nCoin change is the classic unbounded knapsack problem. It models making change, resource allocation with unlimited supplies, and is the introductory problem for DP optimization. The bounded variant (limited coin counts) requires a different approach.`,

  'js-knapsack-01': `The 0/1 knapsack maximizes value within a weight capacity where each item is used at most once. The 2D DP table dp[i][w] represents the best value using the first i items with capacity w, choosing to include or exclude each item.\\n\\nTime is O(n * capacity) and space is O(n * capacity), optimizable to O(capacity) with a single row processed backwards. This is pseudo-polynomial: polynomial in the numeric value of capacity, not in the input size.\\n\\nThe 0/1 knapsack is the most important combinatorial optimization problem. It models portfolio selection, cargo loading, budget allocation, feature selection, and any binary decision problem with weight/cost constraints. It is NP-hard in general but solvable efficiently for moderate capacities.`,

  'js-lis-length': `LIS finds the longest strictly increasing subsequence using DP where dp[i] is the LIS length ending at index i. For each element, check all previous elements and extend the longest compatible subsequence.\\n\\nThe O(n^2) approach checks all pairs. An O(n log n) optimization uses patience sorting (maintaining tails of increasing subsequences with binary search). The DP approach is more intuitive and sufficient for moderate input sizes.\\n\\nLIS appears in patience sorting, envelope nesting (Russian doll), box stacking, longest chain problems, and computing the minimum number of non-increasing subsequences to cover an array. It is a fundamental subsequence problem in interviews.`,

  'js-rod-cutting': `Rod cutting maximizes revenue by trying all possible first cuts and taking the best. The DP relation dp[i] = max(prices[j] + dp[i-j-1]) for all cut lengths j from 0 to i-1 captures the unbounded nature (remaining rod can be cut again).\\n\\nTime is O(n^2) for the nested loops and space is O(n) for the DP array. This is structurally similar to unbounded knapsack where "items" are rod pieces of each length.\\n\\nRod cutting is a classic DP optimization problem that demonstrates bottom-up DP construction. It teaches how to enumerate decisions (where to make the first cut) and combine them with optimal sub-solutions, a pattern that generalizes to many optimization problems.`,

  'js-climbing-stairs': `Climbing stairs with 1 or 2 steps follows the Fibonacci recurrence: dp[i] = dp[i-1] + dp[i-2]. The number of ways to reach step i is the sum of ways to reach the two steps it can be reached from.\\n\\nTime is O(n) and space is O(n), optimizable to O(1) by keeping only the last two values. This is the simplest DP counting problem and serves as the gateway to understanding DP.\\n\\nClimbing stairs generalizes to k-step variants, weighted step costs, and is structurally equivalent to tiling problems (domino tiling), word break counting, and any problem where you count paths in a DAG with fixed step sizes.`,

  'js-unique-paths-grid': `Grid unique paths counts ways to travel from top-left to bottom-right moving only right or down. Each cell dp[i][j] sums the paths from above (dp[i-1][j]) and from the left (dp[i][j-1]).\\n\\nTime and space are O(m * n). The first row and column are all 1s since there is only one path (straight line) to each. The answer equals C(m+n-2, m-1), the binomial coefficient for choosing when to go down.\\n\\nGrid paths is the foundational 2D DP problem that extends to obstacles (set blocked cells to 0), minimum cost paths (take min instead of sum), and robot navigation problems. Understanding the grid DP pattern is essential for many interview problems.`,

  'js-word-break': `Word break determines if a string can be segmented into dictionary words using 1D DP. dp[i] is true if the first i characters can be fully segmented. For each position i, check all split points j where dp[j] is true and s[j..i] is in the dictionary.\\n\\nTime is O(n^2 * L) where L is the maximum word length (for substring creation). Using a Set for the dictionary gives O(1) word lookups. Space is O(n) for the DP array.\\n\\nWord break is a popular interview problem that combines string processing with DP. It is used in NLP tokenization, URL segmentation, and Chinese/Japanese text segmentation where spaces are not present in the input.`,

  'js-max-product-subarray': `Maximum product subarray extends Kadane's algorithm by tracking both running max and min products, because a negative number can flip the minimum to maximum. At each step, consider: start fresh, extend the max, or extend the min.\\n\\nTime is O(n) and space is O(1). The three-candidate approach (current element alone, element * prevMax, element * prevMin) handles all sign combinations correctly.\\n\\nThis problem teaches that negative numbers require tracking auxiliary information (the minimum) alongside the primary quantity (the maximum). The same insight applies to problems involving multiplication, division by negatives, or any operation where the "worst" intermediate result can become the "best."`,

  'js-deep-equals': `Deep equality recursively compares nested objects and arrays by structural matching. Primitives use ===, arrays check length and element-wise equality, and objects check key sets and value-wise equality.\\n\\nTime is O(n) where n is the total number of values across all nesting levels. The type-checking order matters: handle null before typeof "object" since typeof null === "object" in JavaScript.\\n\\nDeep equality is essential for testing frameworks (Jest's toEqual), React's shouldComponentUpdate optimization, state comparison in Redux, and implementing custom equality logic for complex data structures.`,

  'js-merge-intervals': `Merging overlapping intervals sorts by start time, then greedily extends or creates intervals. If the current interval overlaps with the last merged one (current.start <= last.end), extend the end; otherwise start a new interval.\\n\\nSorting takes O(n log n) and the merge pass is O(n), giving O(n log n) overall. The greedy approach is correct because sorting ensures overlapping intervals are adjacent.\\n\\nInterval merging is used in calendar scheduling (finding free time), resource allocation, genomics (merging overlapping gene regions), and log entry consolidation. It is one of the most commonly asked greedy algorithm interview problems.`,

  'js-insert-interval': `Insert interval uses a three-phase approach: copy intervals ending before the new one, merge all overlapping intervals with the new one, then copy remaining intervals. This handles the insertion and merging in a single pass.\\n\\nTime is O(n) for the single pass (no sorting needed since input is already sorted). The merge phase expands the new interval to cover all overlaps by taking min of starts and max of ends.\\n\\nInsert interval builds on merge intervals and tests the ability to handle boundary conditions precisely. It is used in event scheduling systems, adding appointments to calendars, and dynamic interval management.`,

  'js-event-emitter': `An event emitter implements the observer/publish-subscribe pattern with on (register listener), off (remove listener), and emit (notify all listeners). The internal structure maps event names to arrays of callback functions.\\n\\nAll operations are straightforward: on pushes to the listener array, off filters it, and emit iterates and calls each listener. Registration and emission are O(k) where k is the number of listeners for that event.\\n\\nEvent emitters are the backbone of Node.js (EventEmitter class), browser DOM events, React's synthetic event system, Vue's event bus, and any loosely coupled architecture. Understanding this pattern is fundamental to JavaScript programming.`,

  'js-promise-all': `Promise.all resolves when every input promise resolves, collecting results in order, or rejects immediately on the first rejection. A counter tracks resolved promises, and results are stored by index to maintain order.\\n\\nThe implementation wraps each value with Promise.resolve() to handle non-promise values. The counter approach is necessary because promises resolve asynchronously and out of order. Space is O(n) for the results array.\\n\\nPromise.all is fundamental to concurrent JavaScript: loading multiple API endpoints simultaneously, initializing parallel database connections, batch processing, and any scenario where multiple async operations must all complete before proceeding.`,

  'js-promise-race': `Promise.race settles with the first promise to resolve or reject, ignoring all others. The implementation simply routes each promise's resolution and rejection to the outer promise's resolve/reject callbacks.\\n\\nThe simplicity works because a Promise can only be settled once; subsequent calls to resolve or reject are silently ignored. Time to settle is O(min settle time) across all promises.\\n\\nPromise.race implements timeout patterns (race against a setTimeout), competitive fetching (use the fastest mirror), first-response-wins strategies, and is used in Promise.any (ES2021) which resolves with the first fulfillment rather than the first settlement.`,

  'js-deep-freeze': `Deep freeze recursively applies Object.freeze to all nested objects and arrays, making the entire structure immutable. Object.isFrozen prevents re-freezing already-frozen objects, which could cause infinite loops with circular references.\\n\\nTime is O(n) where n is the total number of properties across all nesting levels. Object.freeze is shallow (only freezes own properties), so the recursion is necessary for full immutability.\\n\\nDeep freeze enforces immutability for configuration objects, prevents accidental mutation in shared state (especially useful in development mode for Redux stores), and enables optimizations in frameworks that use referential equality checking for change detection.`,

  'js-object-pick': `Pick creates a new object containing only the specified keys from the source, implemented by iterating over the key array and copying values that exist on the source object using the "in" operator.\\n\\nTime is O(k) where k is the number of keys to pick. The "in" operator checks for key existence without accessing the value, and missing keys are silently skipped.\\n\\nPick is one of the most commonly used utility functions: shaping API responses to return only requested fields, extracting form data for specific inputs, projecting database query results, and implementing GraphQL-style field selection on the server.`,

  'js-object-omit': `Omit creates a new object excluding specified keys, using a Set for O(1) exclusion checks. It iterates over all source keys and includes only those not in the exclusion Set.\\n\\nTime is O(n) where n is the number of source keys, plus O(k) to build the exclusion Set. Space is O(n) for the result and O(k) for the Set.\\n\\nOmit is the complement of pick and is used for removing sensitive fields before logging (passwords, tokens), stripping internal metadata before API responses, data sanitization, and creating derived objects without specific properties.`,

  'js-flat-map': `FlatMap applies a mapping function to each element and flattens the result by one level. If the mapper returns an array, its elements are spread into the result; otherwise the value is pushed directly.\\n\\nTime is O(n * m) where m is the average size of mapped results. FlatMap is equivalent to map followed by flat(1) but avoids creating an intermediate array.\\n\\nFlatMap is a monadic bind operation fundamental to functional programming. It is used in stream processing (RxJS), query building (generating multiple rows from one input), tokenization (splitting strings into words), and is natively available as Array.prototype.flatMap.`,

  'js-run-length-encode': `Run-length encoding compresses consecutive identical characters into count-character pairs. Iterating through the string and flushing a count when the character changes produces the encoded output in O(n) time.\\n\\nThe algorithm uses O(n) space for the output string in the worst case (all unique characters produce "1a1b1c..."). Best case compression occurs with long runs of identical characters.\\n\\nRLE is one of the simplest lossless compression algorithms, used in BMP image format, fax transmission, as a preprocessing step for Burrows-Wheeler compression, and as an introductory string processing problem that teaches the running-count pattern.`,

  'js-run-length-decode': `Run-length decoding parses number-character pairs from an encoded string, repeating each character by its count. Multi-digit number handling requires accumulating digit characters before encountering the letter.\\n\\nTime is O(n) where n is the decoded string length (sum of all counts). The parsing loop distinguishes digits from non-digits to extract the count, then uses String.repeat() for expansion.\\n\\nDecoding is the inverse of encoding and teaches string parsing with state tracking (accumulating digits). The same parse-digits-then-character pattern appears in many format parsers, URL decoders, and protocol message handlers.`,

  'js-debounce-leading': `Leading-edge debounce fires immediately on the first call, then ignores subsequent calls during the delay period. Each call resets the timer, and when it expires, the next call becomes a "first call" again.\\n\\nThe implementation checks if no timer is active (meaning this is a leading-edge call) and invokes immediately. Every call resets the timer. When the timer expires, it is set to null, re-enabling the leading edge.\\n\\nLeading debounce provides instant feedback while preventing rapid-fire execution. It is ideal for button click handlers (instant response, no double-clicks), form submissions, and any interaction where the user expects immediate feedback but repeated actions should be suppressed.`,

  'js-decimal-to-binary': `Decimal to binary conversion repeatedly divides by 2, collecting remainders in reverse order. Each remainder is the next bit from least significant to most significant, so prepending to the result string produces the correct binary representation.\\n\\nThe algorithm runs in O(log n) time (the number of bits in n) with O(log n) space for the output string. The process terminates when the quotient reaches zero.\\n\\nManual base conversion builds understanding of positional number systems fundamental to all computing. It extends to any base (hex, octal), and is the conceptual basis for understanding how computers represent numbers internally.`,

  'js-binary-to-decimal': `Binary to decimal conversion processes bits left-to-right using the accumulator pattern: multiply the running total by 2 and add the current bit. This is Horner's method applied to base-2 polynomial evaluation.\\n\\nTime is O(L) where L is the string length, with O(1) space for the accumulator. The multiply-and-add approach avoids computing powers of 2 explicitly and works for any base by changing the multiplier.\\n\\nHorner's method is the optimal algorithm for polynomial evaluation and base conversion. It appears in hash function computation, parsing numbers from strings in any base, and evaluating mathematical expressions efficiently.`,

  'js-count-bits': `Brian Kernighan's algorithm counts set bits by repeatedly clearing the lowest set bit with n & (n-1). Each iteration removes exactly one bit, so the loop runs exactly k times where k is the number of set bits.\\n\\nThis is O(k) where k is the popcount, better than the naive O(log n) approach of checking each bit. The operation n & (n-1) works because subtracting 1 flips all bits from the lowest set bit downward, and AND clears them.\\n\\nPopcount (population count) is used in cryptographic hash analysis, Hamming distance computation, error detection codes, bitmap index counting, and chess programming (counting pieces on a bitboard). Modern CPUs have a dedicated POPCNT instruction for this.`,

  'js-is-power-of-two': `The power-of-two check uses the bit trick n > 0 && (n & (n-1)) === 0. Powers of 2 have exactly one set bit (1, 10, 100, 1000...), so clearing the lowest set bit with n & (n-1) yields zero if and only if n is a power of 2.\\n\\nThis runs in O(1) time and O(1) space, making it the optimal check. The n > 0 guard is necessary because 0 & (-1) === 0 but zero is not a power of 2.\\n\\nPower-of-two checks are used in memory allocators (ensuring alignment), hash table sizing (powers of 2 enable bitwise modulo), buffer sizing, fast modular arithmetic, and as a building block in other bit manipulation algorithms.`,

  'js-toggle-bit': `Toggling the nth bit uses XOR with a shifted mask: num ^ (1 << n). XOR with 1 flips a bit (0 becomes 1, 1 becomes 0), while XOR with 0 leaves it unchanged, so the mask isolates the target bit.\\n\\nThis runs in O(1) time and O(1) space. The three bit operations (AND for clear, OR for set, XOR for toggle) form the complete toolkit for individual bit manipulation.\\n\\nBit toggling is used in feature flags (toggle features on/off), graphics rendering (XOR drawing for rubber-band selection), permission systems (flip individual permission bits), state machines (toggle between states), and encryption (XOR cipher).`,

  'js-matrix-multiply': `Matrix multiplication computes C[i][j] as the dot product of row i of A and column j of B, requiring three nested loops. The shared dimension (columns of A = rows of B) determines the inner summation.\\n\\nTime complexity is O(m * n * p) for multiplying an m*n matrix by an n*p matrix. Space is O(m * p) for the result. Strassen's algorithm achieves O(n^2.807) for square matrices, but the cubic algorithm is simpler and practical for small matrices.\\n\\nMatrix multiplication is fundamental to linear algebra, computer graphics (transformations), neural network forward passes, graph path counting (A^k gives k-length paths), and scientific computing. Understanding the triple-loop structure is prerequisite for optimization work.`,

  'js-transpose-matrix': `Matrix transpose swaps rows and columns, moving element [i][j] to [j][i]. For an m*n input, the result is n*m. Creating a new result matrix with swapped dimensions and copying elements is straightforward.\\n\\nTime is O(m * n) to copy all elements, and space is O(m * n) for the output. In-place transpose is only possible for square matrices (swapping [i][j] with [j][i] for the upper triangle).\\n\\nTranspose is used as the first step in clockwise rotation, converting row-major to column-major storage, reshaping data frames, and is fundamental to linear algebra operations like computing A^T * A in regression.`,

  'js-object-deep-merge': `Deep merge recursively combines two objects: when both values for a key are plain objects, merge them recursively; otherwise the source value wins. Arrays are replaced, not merged, to avoid ambiguous semantics.\\n\\nTime is O(n) where n is the total number of properties across all nesting levels. The type checks (!Array.isArray and typeof === "object") ensure only plain objects trigger recursion.\\n\\nDeep merge is essential for configuration systems (layered config files), theme customization (override specific nested properties), Redux state reducers (merging partial updates), and building complex objects from defaults and user overrides.`,

  'js-retry-async': `Retry wraps an async function with fault tolerance, re-attempting on failure up to a specified number of times with a delay between attempts. A for loop with try/catch handles the retry logic cleanly.\\n\\nTime depends on the wrapped function and retry count. The delay uses await new Promise(r => setTimeout(r, delay)) to pause between attempts without blocking. If all retries fail, the last error is thrown.\\n\\nRetry logic is essential for resilient API calls, database connection establishment, distributed systems communication, file operations that may fail transiently, and implementing exponential backoff strategies for rate-limited services.`,

  'js-throttle-leading-trailing': `Full-featured throttle fires on both leading edge (immediately) and trailing edge (after interval expires with the latest arguments). This ensures both instant responsiveness and final-state capture.\\n\\nThe implementation stores the most recent arguments during the throttle interval. When the timer fires, if there are saved arguments, it invokes with them and restarts the timer. This gives at most 1/interval invocations per second.\\n\\nLeading+trailing throttle is the production-grade version used in scroll and resize handlers, real-time collaborative editing (send updates at most every 100ms), analytics event batching, and any high-frequency input that needs both immediate feedback and guaranteed final processing.`,
};

// Strategy: Find each exercise-level description (pattern: "    description:\n" standalone),
// then find the end of that description value (line ending with "',"), and insert after.
// Process from BOTTOM to TOP to avoid shifting line indices.

const insertions = []; // [{afterLine: number, text: string}]

let currentId = null;
for (let i = 0; i < lines.length; i++) {
  const trimmed = lines[i].trim();

  // Track current exercise ID
  const idMatch = trimmed.match(/^id:\s*'(js-[^']+)'/);
  if (idMatch) {
    currentId = idMatch[1];
  }

  // Detect exercise-level description: exactly "    description:" at end of line
  if (lines[i].match(/^    description:\s*$/) && currentId) {
    // Find the end of the description string value
    // The next line(s) contain the value. Look for line ending with ',
    for (let j = i + 1; j < Math.min(i + 5, lines.length); j++) {
      const valueLine = lines[j].trimEnd();
      if (valueLine.endsWith("',")) {
        // This is the end of the description value
        if (explanations[currentId]) {
          insertions.push({
            afterLine: j,
            text: `    explanation: \`${explanations[currentId]}\`,`,
          });
        }
        break;
      }
    }
    currentId = null; // Don't match again for this exercise
  }
}

// Insert from bottom to top to preserve line numbers
insertions.sort((a, b) => b.afterLine - a.afterLine);
for (const ins of insertions) {
  lines.splice(ins.afterLine + 1, 0, ins.text);
}

const result = lines.join('\n');
fs.writeFileSync(filePath, result, 'utf8');

const explanationCount = insertions.length;
console.log(`Total explanation fields inserted: ${explanationCount}`);
console.log(`Expected: ${Object.keys(explanations).length}`);

// Check for any missing
const missing = Object.keys(explanations).filter(id =>
  !insertions.some(ins => result.includes(`id: '${id}'`))
);
if (missing.length > 0) {
  console.log(`IDs not found in file: ${missing.join(', ')}`);
}
